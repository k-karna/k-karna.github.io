---
layout: article
title: Basic Statistics
date: 2022-05-13
sidebar:
  nav: "docs-en"
mathjax: true
---


__Population__: Population is a set of similar items or events that is of interest for some statistical problem. It can be a group of existing object or a hypothetical or potentially infinite group of items.<br>
__Sample__: Sample is a selection of individual, events or objects taken from a well-defined population. Thus sample is a subset of population.<br>
__Parameters__: These are those quantities that summarizes or describes an aspect of the population such as mean, deviation, correlation, etc.<br>
__Sampling Error__: It is the difference between sample statistics and population parameters. Since, samples does not include all elements of population, its estimate tends to differ from it.<br>
__Range__ :Range is the difference between the smallest and the largest data value.<br>
__Quartiles, Deciles, Percentiles__: Quartiles divide a set of observations into 4 equal parts. Deciles divide them into 10 equal parts, whereas Percentiles divide observations into 100 equal parts.

$$\text{Location of a Percentile}, L_{p} = (n+1)\frac{P}{100}$$

__Mode__ : Mode is the score that occurs most often in a frequency distribution of data.<br>
__Median__ : Median is the middle score found by arranging a set of numbers from the smallest to the largest (or from largest to smallest). If even number in data point, then median is the average of two middle values.<br>
__Mean__ : Mean is the arithmetic average of numbers in a data set i.e, sum of numbers divided by the total.<br>
__Geometric Mean__: Geometric mean (GM) is the average change of percentage, ratios, indexes or growth rate over time. If we have a set of _n_ positive numbers, then its GM is defined as the _nth_ root of the product of _n_ values.

$$ \text{GM} = \sqrt[n]{(x_{1})(x_{2}) \cdots (x_{n})}$$

It can also be used to get __Rate of Increase Over Time__. In this case, GM is calculated as:

$$\text{GM} = \sqrt[n]{\frac{\text{Value at the end of period}}{\text{Value at start of period}}} -1$$

__Harmonic Mean__ : Harmonic Mean(HM) is the reciprocal of the arithmetic mean, and useful when averages of ratios, and rates are needed. If we have a set of $n$ numbers, then its HM is defined as:

$$HM = \frac{n}{\frac{1}{x_{1}} + \frac{1}{x_{2}} + \cdots + \frac{1}{x_{n}}}$$

---
## Expected Values

Expected value is the weighted average of a random variable according to the probability distribution. It is supposed to be the approximate measure of center of a distribution. Simply put, if we have random variable $X$ with finite list of $x_1, x_2 \cdots x_n$ with probability of occurence as $p_1, p_2 \cdots p_n$, then the Expected Value of X or $E[X] = x_{1}p_1, x_{2}p_2, \cdots x_{n}p_n$<br>
So, the Expected value of a random variable $X$ can be represented as:

$$
E[X] =
\begin{cases}
\sum_{i}^\infty x_{i}p_{i},  & \text{if $X$ is discrete } \\
\int_{-\infty}^{\infty} x f(x) dx, & \text{if $X$ is continuous}
\end{cases}$$

__Note:__ If $X$ is a randon variable, and $a$, $b$ and $c$ are some constants, then for any functions $g_1(x)$ and $g_2(x)$ whose expectations exists:
- $E(ag_{1}(X) + bg_{1}(X) + c) = aEg_{1}(X) + bEg_{1}(X) + c$
- if $g_{1}(X) \ge 0$ for all x, then $Eg_{1}(X)\ge0$
- If $g_1(X) \ge g_2(X)$ for all $x$, then $Eg_{1}(X) \ge Eg_{2}(X)$
- If $a \le g_1(X) \le b$ for all $x$, then $a \le Eg_1(X) \le b$

## **Moments

** <small>We know that first moment is mean($\mu$), second moment is variance. Also,standardized moment is $\frac{\text{Moment}}{\sigma^n}$. We also know that first standardized moment is 0, and second is 1. Second being 1 makes sense as $\frac{var(X)}{\sigma^2}$ will be 1, but first standardized moment $\frac{\mu}{\sigma}≠ 0$. In most books, it is not mentioned, and wherever it is(including wikipedia), it is mentioned as _(raw) first moment is mean, $\mu$ and second central moment is variance_. Casella & Berger said it is _n-th moment of $X$_ : $\mu_{n} = EX^n$ and _n-th central moment of $X$_: $\mu_{n} = E(X - \mu)^n$.<br>
So, there must be two first moment - __raw first moment__ that is Expected value ($EX$) or Mean, $\mu$, and __first central moment__ that is $0$ because $E(X - \mu)^1 = 0$, measuring spread from Expected Value or Mean to mean.</small>

### Variance

Variance is the second central moment of a random variable $X$ and can be given by $E(X - \mu)^2$.<br>
We can write it as:<br>

$$\text{Var} (X) = E[(X - \mu)^2] \qquad \text{or,}\qquad \text{Var}(X) = E[(X - EX)^2]$$

Key things:<br>
- From $\text{Var}(X) = E[(X - EX)^2]$, we can further derive that: $\text{Var}(X) = E[X^2] - E[X]^2$
- Variance of a constant is zero i.e,  $\text{Var} (a) = 0$
- $\text{Var} (X + a) = \text{Var} (X)$
- $\text{Var}(aX + b) = a^2 \text{Var}X$

### Standard Deviation

Standard deviation ($\sigma$) is square root of variance, and is a good measure for having same unit as data, to ascertain if most values are closer to the expected value (by having lower $\sigma$) or farther away by having larger $\sigma$. We can define it as:

$$σ = \sqrt{E[(X - \mu)^2]} \quad \text{or} \qquad\sqrt{\int_{-\infty}^{\infty}(x - \mu)^2 f(x)dx}$$

It can further be written as: $\sqrt{E[X^2] - E[X]^2}$

### Standardized Moment

It is used to make moment scale-invariant. The standardized moment of degree $k$ normalizes __central moment__ of degree $k$ by standard deviation $\sigma^k$.

- First central moment is $0$. Therefore, first standardized moment i.e, $\frac{0}{\sigma^1} = 0$
- Second central moment is variance. Therefore, second standardized moment is given as: $\frac{\text{Variance}}{\sigma^2} = \frac{E[(X - \mu)^2]}{(\sqrt{E[(X - \mu)^2]})^2} = 1$.

### Skewness

### Kurtosis
__3rd Moment -> Skewness__ :

It is a measure of the asymmetry of the probability distribution of data and defined as:

$$γ  = \frac{1}{N \sigma^{3}}\sum_{i=1}^{n} (x_{i}- \mu)^{3}$$

Generally, If $$\text{Mean} \gt \text{Mode}$$, the skewness is positive having _tail_ on the right side, and if $$\text{Mean} \lt \text{Mode}$$, the skewness is negative, with  _tail_ on the left side of the distribution.

![SKEW](/assets/img/skewness_plot.png)

Pearson Skewness Coefficient (based on Mode) is defined as: $$\displaystyle \frac{\text{Mean} - \text{Mode}}{\text{stddev}}$$ and based on Median, it is $$\displaystyle \frac {3 (Mean - Median)}{\text{stddev}}$$

### Kurtosis

It refers to the degree of peakedness of a frequency curve. It tell how tall and sharp the cenral peak is, relative to a standard bell curve of normal distribution. Kurtosis can be described in the following ways:

- Playkurtic : When $$\text{Kurtosis} \lt 0$$. The curve is more flat and wide.

- Leptokurtic : When $$\text{Kurtosis} \gt 0$$. The curve is more peaked.

- Mesokurtic : When the $$\text{Kurtosis} = 0$$ (_normal_ in shape)

![KURT](/assets/img/kurtosis.png)



## Covariance

Covariance between two random vairable, $$x$$ and $$y$$ measures how two variables are related. Positive covariance means the two variables are positively related, and they move in the same direction.

Negative covariance means that the variables are inversely related, or that they move in opposite directions.

$$\text{COV}(x,y) = \frac{\sum_{i=1}^{n} (x -\mu_{x})(y-\mu_{y})}{n-1}$$

where $$\mu_{x}$$ is mean of $$x$$ and $$μ_{y}$$ is mean of $$y$$ and $$n$$ is the total number of data values.

## Tchebysheff Inequality Theorem

It helps determine proportion of observation expected within a certain number of standard deviation from the mean, even if data is not __normally distributed__

Given a number $$k \ge 1$$, and set of $$n$$ measurements, at least, $$1 -\frac{1}{k^{2}}$$ of the measurements will lie within $$k$$ standard deviation of their mean, and following deduction can be made :

Lower limit = mean - $$k ×  \text{std.dev}$$

Upper limit = mean - $$k \times \text{std.dev}$$

## Correlation

It measures the interdependence between two variables and illustrate how closely two variables move together. Correlation value range between $$-1.0$$ and $$1.0$$. A correlation value of $$-1.0$$ represents negative correlation between said variables and they move in opposite direction. A correlation value of $$0$$ means no linear relationship at all. A perfect positive correlation value is 1. below given image is to illustrate it.

![CorrCoeff](/assets/img//Corr_coeff.png)

### Pearson Correlation Coefficient

It is between two <i>linearly</i> related variables, and required three assumption to be true i.e, 1. interval or ratio level, 2. Bivariable normally distributed 3. Linearly related. <br> For sample, it is defined as :

$$\rho=\frac{\sum(x_{i} - \bar{x})(y_{i}- \bar{y})} {\sqrt{\sum(x_{i} - \bar{x})^{2} \sum(y_{i} - \bar{y})^2}}$$


where $$ρ =$$ correlation coefficient,

$$x_{i} = $$values of the x-variable in sample

$$\bar{x}=$$ mean of the values of the x-variable

$$y_{i} = $$ valus of the y-variable in a sample

$$\bar{y}=$$ mean of the values of the y-variable

For a population, correlation coefficient is defined as:

$$\rho_(X,Y)=\frac{\text{cov}(X,Y)}{\sigma_{X} \sigma_{Y}}$$

where $$\text{cov}(x,y) = $$ covariance between $x$ and $y$$

$$\sigma_{x} = $$ standard deviation of $$X$$
$$\sigma_{Y} = $$ standard deviation of $$Y$$

### Spearman's Rank Correlation Coefficient

It requires two assumption to be true i.e, 1. interval or ratio level or ordinal(categorical data) 2. monotonically related. A monotonic function is one that either never increases or never decreases as its independent variable increases. Below given image is to demonstrate that 

![Monotonic](/assets/img/monotonic.png)

__Spearman's Rank Correlation__ is represented by $$r_{s}$$ and constrained as $$-1 \le r_{s} \le 1$$. For a sample size of $$n$$, the $$n$$ raw scores $$X_{i}, Y_{i}$$ are converted to ranks $$R(X_{i}), R(Y_{i})$$ and $$r_{s}$$ then computed as: 

$$ r_{s} = \rho_{R(X),R(Y)} = \frac{cov(R(X),R(Y))}{\sigma_(R(x)) \sigma_(R(Y))}$$

where $$\rho$$ denotes __Pearson Correlation Coefficient__ of rank variables,
$$cov(R(X),R(Y))$$ is covariance of rank variables

$$\sigma_{R(X)}$$, and $$\sigma_{R(Y)}$$ are standard deviations of rank variables.

If all $$n$$ ranks are distinct integers, it can be computed as:

$$ r_{s} = 1 - \frac{6 ∑ d_{i}^2}{n(n^2 - 1)}$$

where $$d_{i} = R(X_{i}) - R(Y_{i})$$ is the difference between the two ranks of each obsevation, and $$n$$ is the number of observations.

### Z-score

It is the number of standard deviation by which the value of a raw score is above or below the mean. Raw score above mean value _positive_ z-score (standard score) and those below mean value have _negative_ z-score (standard score). It is defined as :

$$ z = \frac{x - \mu}{\sigma}$$

where $$\mu$$ is the mean of the population and $$\sigma$$ is Standard Deviation of the population. And, for sample,

$$z = \frac{ x - \bar{x}}{S}$$ 

where $$\bar{x}$$ is the mean of the sample, and $$S$$ is the standard deviation of the sample.